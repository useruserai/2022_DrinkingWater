# -*- coding: utf-8 -*-
"""Untitled96.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ba6vTnmpQlDHsfON9peYOb4s798Vkzj0
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision

!pip install tqdm

with open("/content/drive/MyDrive/Homework5/config/mask_faster_rcnn.yaml", 'r') as file:
    content = file.read()

# 내용 출력
print(content)

# 파일 경로
file_path = "/content/drive/MyDrive/Homework5/config/mask_faster_rcnn.yaml"

# 수정할 내용 (수정된 yaml 내용)
modified_content = """
# 데이터셋
task: Mask
data_files:
  annotation_file: /content/drive/MyDrive/Homework5/data/mask_archive/annotations
  image_file: /content/drive/MyDrive/Homework5/data/mask_archive/images
  annotation_val_file: /content/drive/MyDrive/Homework5/data/mask_archive/val_annotations
  image_val_file: /content/drive/MyDrive/Homework5/data/mask_archive/val_images
  annotation_test_file: /content/drive/MyDrive/Homework5/data/mask_archive/test_annotations
  image_test_file: /content/drive/MyDrive/Homework5/data/mask_archive/test_images

batch_size: 4

# 초기화
random_seed: 54321

# 레귤러리제이션
l2_reg_lambda: 5.0e-4

momentum: 0.9
lr: 5.0e-3

# 트레이닝 프로세스
max_epochs: 20
"""

# 수정된 내용을 파일에 다시 쓰기
with open(file_path, 'w') as file:
    file.write(modified_content)

print("파일이 성공적으로 수정되었습니다.")

import sys
sys.path.append('/content/drive/MyDrive/Homework5')

# Colab에서 '__init__.py' 파일을 생성하는 코드
with open('/content/drive/MyDrive/Homework5/utils/__init__.py', 'w') as f:
    pass  # 내용은 비워두면 됨

print(sys.path)

!ls /content/drive/MyDrive/Homework5/utils/

!pip install datasets

import time
import sys
import yaml
import random
import os
import tqdm
import torchvision
from torchvision import transforms
import torch
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.utils.tensorboard import SummaryWriter
from utils.data_prepro import MaskDataset, collate_fn
from utils.metrics import get_batch_statistics, ap_per_class

def main():

    params_filename = '/content/drive/MyDrive/Homework5/config/mask_faster_rcnn.yaml'

    with open(params_filename, 'r', encoding="UTF8") as f:
        params = yaml.safe_load(f)

    # 랜덤 시드 세팅
    if 'random_seed' in params:
        seed = params['random_seed']
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)


    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print(device)
    torch.backends.cudnn.benchmark = True

    # 데이터 로드
    if params['task'] == "Mask":
        # 데이터 개수
        print('train 데이터 annotations 수 : {}'.format(len(os.listdir(params['data_files']['annotation_file']))))
        print('train 데이터 images 수 : {}'.format(len(os.listdir(params['data_files']['image_file']))))
        print('val 데이터 annotations 수 : {}'.format(len(os.listdir(params['data_files']['annotation_val_file']))))
        print('val 데이터 images 수 : {}'.format(len(os.listdir(params['data_files']['image_val_file']))))

        data_transform = transforms.Compose([  # transforms.Compose : list 내의 작업을 연달아 할 수 있게 호출하는 클래스
            transforms.ToTensor()  # ToTensor : numpy 이미지에서 torch 이미지로 변경
        ])

        train_data = MaskDataset(data_transform, params['data_files']['image_file'], params['data_files']['annotation_file'])
        val_data = MaskDataset(data_transform, params['data_files']['image_val_file'], params['data_files']['annotation_val_file'])

    # 배치 단위로 네트워크에 데이터를 넘겨주는 Data loader
    train_loader = torch.utils.data.DataLoader(train_data, params['batch_size'], collate_fn=collate_fn)
    dev_loader = torch.utils.data.DataLoader(val_data, params['batch_size'], collate_fn=collate_fn)

    # 학습 모델 생성

    def get_model_instance_segmentation(num_classes):
        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
        # get number of input features for the classifier
        in_features = model.roi_heads.box_predictor.cls_score.in_features
        # replace the pre-trained head with a new one
        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

        return model

    model = get_model_instance_segmentation(4).to(device) # 모델을 지정한 device로 올려줌

    optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'], weight_decay=params['l2_reg_lambda'])  # model.parameters -> 가중치 w들을 의미

    timestamp = str(int(time.time()))
    out_dir = os.path.abspath((os.path.join(os.path.curdir, "runs", timestamp)))
    checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))
    summary_dir = os.path.join(out_dir, "summaries")

    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    writer = SummaryWriter(summary_dir) # TensorBoard를 위한 초기화
     # training 시작
    start_time = time.time()
    global_steps = 0
    highest_val_mAP = 0.0
    print('========================================')
    print("Start training...")
    for epoch in range(params['max_epochs']):
        train_loss = 0
        train_batch_cnt = 0
        model.train()
        for imgs, annotations in train_loader:

            imgs = list(img.to(device) for img in imgs)
            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]
            loss_dict = model(imgs, annotations)
            losses = sum(loss for loss in loss_dict.values())

            train_loss += losses.item()
            train_batch_cnt += 1

            losses.backward()# 가중치 w에 대해 loss를 미분
            optimizer.step()# 가중치들을 업데이트
            optimizer.zero_grad()

            writer.add_scalar("Batch/Loss", losses.item(), global_steps)

            global_steps += 1
            if (global_steps) % 100 == 0:
                print('Epoch [{}], Step [{}], Loss: {:.4f}'.format(epoch+1, global_steps, losses.item()))

        train_ave_loss = train_loss / train_batch_cnt # 학습 데이터의 평균 loss
        training_time = (time.time() - start_time) / 60
        writer.add_scalar("Train/Loss", train_ave_loss, epoch)
        print('========================================')
        print("epoch:", epoch + 1, "/ global_steps:", global_steps)
        print("training dataset average loss: %.3f" % train_ave_loss)
        print("training_time: %.2f minutes" % training_time)

        model.eval()
        def make_prediction(model, img, threshold):
            preds = model(img)
            for id in range(len(preds)):
                idx_list = []

                for idx, score in enumerate(preds[id]['scores']):
                    if score > threshold:
                        idx_list.append(idx)

                preds[id]['boxes'] = preds[id]['boxes'][idx_list].cpu()
                preds[id]['labels'] = preds[id]['labels'][idx_list].cpu()
                preds[id]['scores'] = preds[id]['scores'][idx_list].cpu()

            return preds

        # validation (for early stopping)
        labels = []
        preds_adj_all = []
        annot_all = []

        for imgs, annotations in dev_loader:
            imgs = list(img.to(device) for img in imgs)

            for t in annotations:
                labels += t['labels']

            with torch.no_grad():
                preds_adj = make_prediction(model, imgs, 0.5)
                preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]
                preds_adj_all.append(preds_adj)
                annot_all.append(annotations)

        sample_metrics = []
        for batch_i in range(len(preds_adj_all)):
            sample_metrics += get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5)

        true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐
        precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels,
                                                           torch.tensor(labels))
        print(AP)
        val_mAP = torch.mean(AP)
        print(f'val_mAP : {val_mAP}')
        writer.add_scalar("Val/mAP", val_mAP, epoch)

        if val_mAP > highest_val_mAP:  # validation accuracy가 경신될 때
            save_path = checkpoint_dir + '/epoch_' + str(epoch + 1) + '.pth'
            torch.save({'epoch': epoch + 1,
                        'model_state_dict': model.state_dict()},
                       save_path)

            save_path = checkpoint_dir + '/best.pth'
            torch.save({'epoch': epoch + 1,
                        'model_state_dict': model.state_dict()},
                       save_path)  # best accuracy에 도달할 때만 모델을 저장함으로써 early stopping
            highest_val_mAP = val_mAP
        epoch += 1


if __name__ == '__main__':
    main()

import torch
import yaml
import sys
import os
from tqdm import tqdm

import torchvision
import torchvision.transforms as transforms
from torch.utils.data import TensorDataset
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from utils.data_prepro import MaskDataset, collate_fn
from utils.metrics import get_batch_statistics, ap_per_class

def main():

    params_filename = '/content/drive/MyDrive/Homework5/config/mask_faster_rcnn.yaml'

    with open(params_filename, 'r', encoding="UTF8") as f:
        params = yaml.safe_load(f)

    # GPU 사용이 가능하면 사용하고, 불가능하면 CPU 활용
    print("GPU Available:", torch.cuda.is_available())
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("device:", device)

    timestamp = "1746369938"
    out_dir = os.path.abspath((os.path.join(os.path.curdir, "runs", timestamp)))

    # 데이터 로드
    if params['task'] == "Mask":
        print('test 데이터 annotations 수 : {}'.format(len(os.listdir(params['data_files']['annotation_test_file']))))
        print('test 데이터 images 수 : {}'.format(len(os.listdir(params['data_files']['image_test_file']))))

        data_transform = transforms.Compose([  # transforms.Compose : list 내의 작업을 연달아 할 수 있게 호출하는 클래스
            transforms.ToTensor()  # ToTensor : numpy 이미지에서 torch 이미지로 변경
        ])

        test_data = MaskDataset(data_transform, params['data_files']['image_test_file'],
                                         params['data_files']['annotation_test_file'])


    test_loader = torch.utils.data.DataLoader(test_data, params['batch_size'], collate_fn=collate_fn)
    # 학습 모델 생성

    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)
    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 4)


    # 저장된 state 불러오기
    checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints/best.pth"))

    # TODO : 세팅값 마다 save_path를 바꾸어 로드
    checkpoint = torch.load(checkpoint_dir)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    def make_prediction(model, img, threshold):
        preds = model(img)
        for id in range(len(preds)):
            idx_list = []

            for idx, score in enumerate(preds[id]['scores']):
                if score > threshold:
                    idx_list.append(idx)

            preds[id]['boxes'] = preds[id]['boxes'][idx_list].cpu()
            preds[id]['labels'] = preds[id]['labels'][idx_list].cpu()
            preds[id]['scores'] = preds[id]['scores'][idx_list].cpu()

        return preds

    labels = []
    preds_adj_all = []
    annot_all = []

    for imgs, annotations in tqdm(test_loader, position=0, leave=True):
        imgs = list(img.to(device) for img in imgs)

        for t in annotations:
            labels += t['labels']

        with torch.no_grad():
            preds_adj = make_prediction(model, imgs, 0.5)
            preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]
            preds_adj_all.append(preds_adj)
            annot_all.append(annotations)

    sample_metrics = []
    for batch_i in range(len(preds_adj_all)):
        sample_metrics += get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5)

    true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐
    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))
    mAP = torch.mean(AP)
    print(f'mAP : {mAP}')

if __name__ == "__main__":
    main()

import torch
import yaml
import sys
import os
from tqdm import tqdm
import time  # 시간 측정을 위한 모듈

import torchvision
import torchvision.transforms as transforms
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from PIL import Image

from utils.data_prepro import plot_image_from_output

def main():
    print('Mask Inference for Faster R-CNN')

    params_filename = '/content/drive/MyDrive/Homework5/config/mask_faster_rcnn.yaml'

    with open(params_filename, 'r', encoding="UTF8") as f:
        params = yaml.safe_load(f)

    # GPU 사용이 가능하면 사용하고, 불가능하면 CPU 활용
    print("GPU Available:", torch.cuda.is_available())
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)

    # 모델 로드
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 4)  # 클래스 수에 맞게 수정

    # 저장된 모델 불러오기
    checkpoint_dir = '/content/runs/1746369938/checkpoints/best.pth'
    checkpoint = torch.load(checkpoint_dir)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    # 단일 이미지 로드 및 전처리
    image_path = '/content/마스크 사진.jpg'
    image = Image.open(image_path).convert("RGB")

    # 이미지 전처리
    transform = transforms.Compose([
        transforms.ToTensor()  # 이미지를 Tensor로 변환
    ])
    img = transform(image).unsqueeze(0).to(device)  # 배치 차원 추가

    # 추론 함수
    def make_prediction(model, img, threshold):
        preds = model(img)
        for id in range(len(preds)):
            idx_list = []

            for idx, score in enumerate(preds[id]['scores']):
                if score > threshold:
                    idx_list.append(idx)

            preds[id]['boxes'] = preds[id]['boxes'][idx_list].cpu()
            preds[id]['labels'] = preds[id]['labels'][idx_list].cpu()
            preds[id]['scores'] = preds[id]['scores'][idx_list].cpu()

        return preds

    # 추론 시간 측정 시작
    start_time = time.time()

    with torch.no_grad():
        preds = make_prediction(model, img, 0.5)

        # 추론 결과 출력
        print("Prediction labels:", preds[0]['labels'])
        print("Prediction boxes:", preds[0]['boxes'])  # 좌상단, 우하단
        plot_image_from_output(img[0], preds[0])  # 예측 결과를 시각화

    # 추론 시간 측정 종료
    end_time = time.time()
    print(f"Inference time: {end_time - start_time} seconds")

if __name__ == "__main__":
    main()

!pip install tensorboard

import os
import xml.etree.ElementTree as ET

# 경로 설정
annotation_dir_test = '/content/drive/MyDrive/Homework5/data/mask_archive/test_annotations'
annotation_dir_val = '/content/drive/MyDrive/Homework5/data/mask_archive/val_annotations'
output_dir_test = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/test'
output_dir_val = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/val'

# 클래스 이름과 클래스 ID 매핑
class_map = {
    'with_mask': 0,  # 'with_mask' -> 0
    'mask_weared_incorrect': 1,  # 'mask_weared_incorrect' -> 1
    'without_mask': 2,  # 'without_mask' -> 2
}

# YOLO용 텍스트 파일 생성 함수
def create_yolo_annotation(xml_file, output_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    # 이미지 크기 가져오기 (XML에 포함된 이미지 크기 사용)
    image_width = int(root.find('size').find('width').text)
    image_height = int(root.find('size').find('height').text)

    with open(output_file, 'w') as f:
        for obj in root.findall('object'):
            # 클래스 이름 가져오기
            class_name = obj.find('name').text
            # YOLO에서 클래스 이름을 숫자로 변환하는 매핑
            class_id = class_map.get(class_name, None)
            if class_id is None:
                print(f"Warning: Class {class_name} not found in class_map!")
                continue  # 매핑이 없는 클래스는 건너뜀

            # bounding box 정보 가져오기
            bndbox = obj.find('bndbox')
            xmin = int(bndbox.find('xmin').text)
            ymin = int(bndbox.find('ymin').text)
            xmax = int(bndbox.find('xmax').text)
            ymax = int(bndbox.find('ymax').text)

            # YOLO 포맷: class_id, x_center, y_center, width, height (정규화된 값)
            x_center = ((xmin + xmax) / 2) / image_width
            y_center = ((ymin + ymax) / 2) / image_height
            width = (xmax - xmin) / image_width
            height = (ymax - ymin) / image_height

            # YOLO 형식으로 파일에 기록
            f.write(f"{class_id} {x_center} {y_center} {width} {height}\n")

# YOLO 텍스트 파일 생성 전에 기존 파일 삭제 (폴더 비우기)
def clear_folder(folder_path):
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if os.path.isfile(file_path) and filename.endswith('.txt'):
            os.remove(file_path)
            print(f"Deleted: {file_path}")

# test_annotations에서 YOLO 텍스트 파일 생성
clear_folder(output_dir_test)  # 기존 test 폴더 비우기
for xml_file in os.listdir(annotation_dir_test):
    if xml_file.endswith('.xml'):
        image_name = xml_file.replace('.xml', '.png')  # 이미지 이름
        output_file = os.path.join(output_dir_test, image_name.replace('.png', '.txt'))
        xml_path = os.path.join(annotation_dir_test, xml_file)
        create_yolo_annotation(xml_path, output_file)
        print(f"Created: {output_file}")

# val_annotations에서 YOLO 텍스트 파일 생성
clear_folder(output_dir_val)  # 기존 val 폴더 비우기
for xml_file in os.listdir(annotation_dir_val):
    if xml_file.endswith('.xml'):
        image_name = xml_file.replace('.xml', '.png')  # 이미지 이름
        output_file = os.path.join(output_dir_val, image_name.replace('.png', '.txt'))
        xml_path = os.path.join(annotation_dir_val, xml_file)
        create_yolo_annotation(xml_path, output_file)
        print(f"Created: {output_file}")

import os

# 각 폴더 경로 설정
test_folder = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/test'
val_folder = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/val'

# test 폴더 내 txt 파일 개수 확인
test_files = [f for f in os.listdir(test_folder) if f.endswith('.txt')]
print(f"test 폴더에 있는 txt 파일 개수: {len(test_files)}")

# val 폴더 내 txt 파일 개수 확인
val_files = [f for f in os.listdir(val_folder) if f.endswith('.txt')]
print(f"val 폴더에 있는 txt 파일 개수: {len(val_files)}")

import os
import xml.etree.ElementTree as ET

# 경로 설정
annotation_dir_train = '/content/drive/MyDrive/Homework5/data/mask_archive/annotations'
output_dir_train = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/train'

# YOLO 텍스트 파일이 저장될 폴더가 없다면 생성
if not os.path.exists(output_dir_train):
    os.makedirs(output_dir_train)

# 클래스 이름과 ID 매핑
class_mapping = {
    'with_mask': 0,
    'mask_worn_incorrect': 1,
    'without_mask': 2
}

# YOLO용 텍스트 파일 생성 함수
def create_yolo_annotation(xml_file, output_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    # 이미지 크기 가져오기 (xml에서 직접 추출)
    image_width = int(root.find('size/width').text)
    image_height = int(root.find('size/height').text)

    with open(output_file, 'w') as f:
        for obj in root.findall('object'):
            # 클래스 이름 가져오기
            class_name = obj.find('name').text
            # 클래스 이름을 ID로 변환
            class_id = class_mapping.get(class_name, -1)
            if class_id == -1:
                continue  # 정의되지 않은 클래스는 건너뜁니다

            # bounding box 정보 가져오기
            bndbox = obj.find('bndbox')
            xmin = int(bndbox.find('xmin').text)
            ymin = int(bndbox.find('ymin').text)
            xmax = int(bndbox.find('xmax').text)
            ymax = int(bndbox.find('ymax').text)

            # YOLO 포맷: class_id, x_center, y_center, width, height (정규화된 값)
            x_center = ((xmin + xmax) / 2) / image_width
            y_center = ((ymin + ymax) / 2) / image_height
            width = (xmax - xmin) / image_width
            height = (ymax - ymin) / image_height

            # YOLO 형식으로 파일에 기록
            f.write(f"{class_id} {x_center} {y_center} {width} {height}\n")

# train_annotations에서 YOLO 텍스트 파일 생성
for xml_file in os.listdir(annotation_dir_train):
    if xml_file.endswith('.xml'):
        image_name = xml_file.replace('.xml', '.png')  # 이미지 이름
        output_file = os.path.join(output_dir_train, image_name.replace('.png', '.txt'))
        xml_path = os.path.join(annotation_dir_train, xml_file)
        create_yolo_annotation(xml_path, output_file)
        print(f"Created: {output_file}")

import os

# yolo_annotations/train 폴더 경로
train_dir = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/train'

# 폴더 내의 .txt 파일 개수 확인
txt_files = [f for f in os.listdir(train_dir) if f.endswith('.txt')]

# 결과 출력
print(f"Total number of .txt files in {train_dir}: {len(txt_files)}")

import os

# yolo_annotations/train 폴더 경로
train_dir = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/val'

# 폴더 내의 .txt 파일 개수 확인
txt_files = [f for f in os.listdir(train_dir) if f.endswith('.txt')]

# 결과 출력
print(f"Total number of .txt files in {train_dir}: {len(txt_files)}")

import os

# yolo_annotations/train 폴더 경로
train_dir = '/content/drive/MyDrive/Homework5/data/mask_archive/yolo_annotations/test'

# 폴더 내의 .txt 파일 개수 확인
txt_files = [f for f in os.listdir(train_dir) if f.endswith('.txt')]

# 결과 출력
print(f"Total number of .txt files in {train_dir}: {len(txt_files)}")